{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632a649d",
   "metadata": {},
   "source": [
    "# Simple HuggingFace inference with Huggingface Adapted FMS models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1500f8",
   "metadata": {},
   "source": [
    "*Note: This notebook is using Torch 2.1.0 and Transformers 4.35.0.dev0*\n",
    "\n",
    "If you would like to run a similar pipeline using a script, please view the following file: `scripts/hf_compile_example.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36289ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dwertheimer/miniconda3/envs/python3/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/dwertheimer/miniconda3/envs/python3/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'triton'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline, AutoTokenizer, AutoModelForCausalLM\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_hf_api\n",
      "File \u001b[0;32m~/Downloads/fmsrepo/sandbox/foundation-model-stack-sandbox/fms/models/__init__.py:326\u001b[0m\n\u001b[1;32m    321\u001b[0m         fms_model \u001b[38;5;241m=\u001b[39m model_wrap(fms_model)\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fms_model\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gpt_bigcode, llama, mixtral, roberta\n",
      "File \u001b[0;32m~/Downloads/fmsrepo/sandbox/foundation-model-stack-sandbox/fms/models/gpt_bigcode.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstrategy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistributedStrategy, NoOpStrategy\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiHeadAttention\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeedforward\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeedForwardBlock\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m str_to_activation\n",
      "File \u001b[0;32m~/Downloads/fmsrepo/sandbox/foundation-model-stack-sandbox/fms/modules/feedforward.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed_c10d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProcessGroup\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmoe_kernel\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmoe_kernel\u001b[39;00m  \u001b[38;5;66;03m# registers the PT custom ops\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distributed\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     copy_to_tensor_model_parallel_region,\n\u001b[1;32m     13\u001b[0m     reduce_from_tensor_model_parallel_region,\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/Downloads/fmsrepo/sandbox/foundation-model-stack-sandbox/fms/triton/moe_kernel.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tuple\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import-untyped]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtl\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import-untyped]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"Fused MoE kernel.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'triton'"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from fms.models import get_model\n",
    "from fms.models.hf import to_hf_api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b5344",
   "metadata": {},
   "source": [
    "## load Huggingface Adapted FMS model\n",
    "\n",
    "Simply get the Huggingface model and convert it to an equivalent HF adapted FMS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b02ec29-289e-425a-960e-a66d6521730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = \"llama\"\n",
    "variant = \"llama2_1.4b\"\n",
    "model_path = \"/Users/dwertheimer/Downloads/oc_work/downloads/llama2-base.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa0364d",
   "metadata": {},
   "source": [
    "If you intend to use half tensors, you must set the default device to cuda and default dtype to half tensors prior to loading the model to save space in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c92721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_default_device(\"cuda\")\n",
    "torch.set_default_dtype(torch.half)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f05d812",
   "metadata": {},
   "source": [
    "get the model and wrap in huggingface adapter api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af00ae-b041-4e2f-b882-c99389d967cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributed._shard.checkpoint import (\n",
    "    FileSystemReader,\n",
    "    FileSystemWriter,\n",
    "    load_state_dict,\n",
    "    save_state_dict,\n",
    ")\n",
    "from torch.distributed.checkpoint.default_planner import (\n",
    "    DefaultLoadPlanner,\n",
    "    DefaultSavePlanner,\n",
    ")\n",
    "from torch.distributed.fsdp import FullStateDictConfig\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp import StateDictType\n",
    "\n",
    "from fms.models.llama import LLaMA, LLaMAConfig\n",
    "\n",
    "c = LLaMAConfig()\n",
    "c.kvheads=8\n",
    "c.nlayers=16\n",
    "c.hidden_grow_factor=3\n",
    "c.emb_dim=2048\n",
    "c.nheads=16\n",
    "\n",
    "model = LLaMA(c)\n",
    "\n",
    "print(\"Model built!\")\n",
    "\n",
    "# model = get_model(architecture, variant, model_path=model_path, source=\"fms\", device_type=\"cpu\", norm_eps=1e-6)\n",
    "# model = to_hf_api(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5318e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cc0d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.load(model_path)['model_state']\n",
    "d = {k[10:]:v for k,v in d.items()}\n",
    "model.load_state_dict(d)\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de0703e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e616a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa871d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf0879e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cc14f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributed import init_process_group\n",
    "import os\n",
    "\n",
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '29500'\n",
    "init_process_group(\"gloo\", rank=0, world_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "251f7d69",
   "metadata": {},
   "outputs": [
    {
     "ename": "CheckpointException",
     "evalue": "CheckpointException ranks:dict_keys([0])\nTraceback (most recent call last): (RANK 0)\n  File \"/Users/dwertheimer/miniconda3/envs/python3/lib/python3.8/site-packages/torch/distributed/checkpoint/utils.py\", line 173, in reduce_scatter\n    local_data = map_fun()\n  File \"/Users/dwertheimer/miniconda3/envs/python3/lib/python3.8/site-packages/torch/distributed/checkpoint/state_dict_loader.py\", line 150, in local_step\n    metadata = storage_reader.read_metadata()\n  File \"/Users/dwertheimer/miniconda3/envs/python3/lib/python3.8/site-packages/torch/distributed/checkpoint/filesystem.py\", line 498, in read_metadata\n    return pickle.load(metadata_file)\nAttributeError: Can't get attribute '_MEM_FORMAT_ENCODING' on <module 'torch.distributed.checkpoint.metadata' from '/Users/dwertheimer/miniconda3/envs/python3/lib/python3.8/site-packages/torch/distributed/checkpoint/metadata.py'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCheckpointException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[1;32m      3\u001b[0m model_ckp \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: state_dict}\n\u001b[0;32m----> 4\u001b[0m \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_ckp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_reader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFileSystemReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplanner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDefaultLoadPlanner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/distributed/checkpoint/state_dict_loader.py:31\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(state_dict, storage_reader, process_group, coordinator_rank, no_dist, planner)\u001b[0m\n\u001b[1;32m     27\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will be removed in future versions. Please use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# TODO: test returning `load` here instead.\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_reader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoordinator_rank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplanner\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/distributed/checkpoint/state_dict_loader.py:164\u001b[0m, in \u001b[0;36m_load_state_dict\u001b[0;34m(state_dict, storage_reader, process_group, coordinator_rank, no_dist, planner)\u001b[0m\n\u001b[1;32m    161\u001b[0m     all_local_plans \u001b[38;5;241m=\u001b[39m storage_reader\u001b[38;5;241m.\u001b[39mprepare_global_plan(all_local_plans)\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_local_plans\n\u001b[0;32m--> 164\u001b[0m central_plan \u001b[38;5;241m=\u001b[39m \u001b[43mdistW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_scatter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_data\u001b[39m():\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m planner \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/distributed/checkpoint/utils.py:200\u001b[0m, in \u001b[0;36m_DistWrapper.reduce_scatter\u001b[0;34m(self, step, map_fun, reduce_fun)\u001b[0m\n\u001b[1;32m    198\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscatter_object(all_results)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, CheckpointException):\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m result\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mCheckpointException\u001b[0m: CheckpointException ranks:dict_keys([0])\nTraceback (most recent call last): (RANK 0)\n  File \"/Users/dwertheimer/miniconda3/envs/python3/lib/python3.8/site-packages/torch/distributed/checkpoint/utils.py\", line 173, in reduce_scatter\n    local_data = map_fun()\n  File \"/Users/dwertheimer/miniconda3/envs/python3/lib/python3.8/site-packages/torch/distributed/checkpoint/state_dict_loader.py\", line 150, in local_step\n    metadata = storage_reader.read_metadata()\n  File \"/Users/dwertheimer/miniconda3/envs/python3/lib/python3.8/site-packages/torch/distributed/checkpoint/filesystem.py\", line 498, in read_metadata\n    return pickle.load(metadata_file)\nAttributeError: Can't get attribute '_MEM_FORMAT_ENCODING' on <module 'torch.distributed.checkpoint.metadata' from '/Users/dwertheimer/miniconda3/envs/python3/lib/python3.8/site-packages/torch/distributed/checkpoint/metadata.py'>\n"
     ]
    }
   ],
   "source": [
    "# with FSDP.state_dict_type(model, StateDictType.SHARDED_STATE_DICT):\n",
    "state_dict = model.state_dict()\n",
    "model_ckp = {\"model_state\": state_dict}\n",
    "load_state_dict(\n",
    "    state_dict=model_ckp,\n",
    "    storage_reader=FileSystemReader(model_path),\n",
    "    planner=DefaultLoadPlanner(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c816c",
   "metadata": {},
   "source": [
    "## Simple inference with Huggingface pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01bf9ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "149e9ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I believe the meaning of life is to find your purpose and to fulfill it.\\n\\nI believe that everyone has a unique purpose in life, and that'}]\n",
      "1.14 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "pipe = pipeline(task=\"text-generation\", model=model, max_new_tokens=25, tokenizer=tokenizer, device=\"cuda\")\n",
    "prompt = \"\"\"I believe the meaning of life is\"\"\"\n",
    "result = pipe(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb08e175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cb14521",
   "metadata": {},
   "source": [
    "## Compilation\n",
    "\n",
    "All fms models support torch compile for faster inference, therefore Huggingface Adapted FMS models also support this feature. \n",
    "\n",
    "*Note: `generate` calls the underlying decoder and not the model itself, which requires compiling the underlying decoder.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a24655fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decoder = torch.compile(model.decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdfd3ce",
   "metadata": {},
   "source": [
    "Because compile is lazy, we first just do a single generation pipeline to compile the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa3c43f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=model, max_new_tokens=25, tokenizer=tokenizer, device=\"cuda\")\n",
    "prompt = \"\"\"I believe the meaning of life is\"\"\"\n",
    "result = pipe(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55206d51",
   "metadata": {},
   "source": [
    "At this point, the graph should be compiled and we can get proper performance numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b22dc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I believe the meaning of life is to find your purpose and to fulfill it.\\n\\nI believe that everyone has a unique purpose in life, and that'}]\n",
      "648 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "pipe = pipeline(task=\"text-generation\", model=model, max_new_tokens=25, tokenizer=tokenizer, device=\"cuda\")\n",
    "prompt = \"\"\"I believe the meaning of life is\"\"\"\n",
    "result = pipe(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f44374",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
