import abc
from typing import Dict, List, Optional, Set, Tuple

import torch
import torch.distributed
from torch import Tensor, nn
from torch.distributed.distributed_c10d import ProcessGroup
from torch.nn import functional as F

from fms import distributed
from fms.distributed.tensorparallel import (
    copy_to_tensor_model_parallel_region,
    reduce_from_tensor_model_parallel_region,
)
from fms.modules.layernorm import LayerNormParameterized
from fms.modules.positions import PositionEncoder
from fms.modules.tp import TPModule


def get_scan_plan(device, n, fmap, h):
    # x: b n d
    # plan: for each level, which entries to avg from previous level ([l] n' 2)
    # inds: which level and entry to pull from in populating heads (n h 2)
    
    # Form ruler-tick progression sequence
    levels = sum(
        [
            torch.arange(n, device=device)
            .remainder(2**i)
            .sub(2**i - 1)
            .sign()
            .add(1)
            for i in range(n.bit_length())
        ]
    ).roll(1, 0)
    if n==65:
        print(levels)
    plan = [
        torch.zeros(0, 2, device=device, dtype=torch.int)
        for _ in range(len(fmap) + 2)
    ]  # [l] 0 2
    plan[1] = (
        torch.arange(n + 1, device=device, dtype=torch.int)
        .unsqueeze(1)
        .expand(-1, 2)
    )
    inds = torch.zeros(n, h, 2, device=device, dtype=torch.long)  # n h 2
    inds[:, 0, 1] = torch.arange(n, device=inds.device, dtype=inds.dtype) + 1
    inds[:, :, 0] = 1
    for i in range(1, n):
        ran = levels[i].item()
        m = fmap.get(ran, h)
        inds[i, 1:m] = inds[i - 1, : m - 1]
        if m < h:
            inds[i, m + 1 :] = inds[i - 1, m + 1 :]
            prev = inds[i - 1, m - 1 : m + 1].flip([0])  # 2 2
            # assert prev[0, 0] == min(levels[i], len(fmap) + 1) or prev[0, 1] == 0, (
            #     levels[i],
            #     prev[0, 0],
            # )
            # assert prev[1, 0] == min(levels[i], len(fmap) + 1) or prev[1, 1] == 0, (
            #     levels[i],
            #     prev[1, 0],
            # )
            level = plan[levels[i] + 1]
            inds[i, m, 0] = levels[i] + 1
            inds[i, m, 1] = level.size(0)
            plan[levels[i] + 1] = torch.cat(
                [plan[levels[i] + 1], prev[:, 1][None]], dim=0
            )
    return plan, inds


def shrink_plan(plan, inds, l):
    # plan: for each level, which entries to avg from previous level ([h] n' 2)
    # inds: which level and entry to pull from in populating heads (n h 2)
    
    # Get modified recursive sum lens
    # First entry is empty, second is seq len plus one for the zero vector entry
    # Subsequent entries are 0 up to 2**(i-2), then increment every 2**(i-1), as seq len increases
    lens = [0,l+1] + [(l-1+2**(i-2))//2**(i-1) for i in range(2,8)]
            
    # Slim down the plan and imap to desired l
    plan = [p[:l] for p,l in zip(plan,lens)]
    inds = inds[:l]

    # Flatten inds (indexing into flattened plan/cache) (n h)
    ls = [p.size(0) for p in plan]
    ls = [0] + ls[:-1]
    offset = torch.tensor(ls, device=inds.device).cumsum(0)
    offset = offset[inds[:, :, 0]]
    inds = offset + inds[:, :, 1]
    return plan, inds


class QKV(nn.Module, metaclass=abc.ABCMeta):
    """Simple module for applying qkv in attention"""

    def __init__(
        self,
        emb_dim: int,
        nheads: int,
        kvheads: int,
        emb_kq_per_head: int,
        emb_v_per_head: int,
        use_bias: bool,
        *args,
        **kwargs,
    ):
        super().__init__(*args, **kwargs)
        self.emb_dim = emb_dim
        self.nheads = nheads
        self.kvheads = kvheads
        self.emb_kq_per_head = emb_kq_per_head
        self.emb_v_per_head = emb_v_per_head
        self.use_bias = use_bias

    @abc.abstractmethod
    def forward(
        self, q: torch.Tensor, k: Optional[torch.Tensor], v: Optional[torch.Tensor]
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """applies query/key/value transformations on q, k, v inputs respectively and returns the resulting values

        Args:
            q: torch.Tensor
                the query tensor
            k: Optional[torch.Tensor]
                the optional key tensor
            v: Optional[torch.Tensor]
                the optional value tensor

        Returns:
        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]
            the query, key, and value computed
        """
        pass

    @abc.abstractmethod
    def reset_parameters(self):
        """resets the query, key, and value weights for training

        Args:
            gain: int
                gain for std in norm (default is 1)
        """
        pass


class UnfusedQKV(QKV):
    """
    Unfused Weights implementation of QKV
    """

    def __init__(
        self,
        emb_dim: int,
        nheads: int,
        kvheads: int,
        emb_kq_per_head: int,
        emb_v_per_head: int,
        use_bias: bool,
        *args,
        **kwargs,
    ):
        super().__init__(
            emb_dim,
            nheads,
            kvheads,
            emb_kq_per_head,
            emb_v_per_head,
            use_bias,
            *args,
            **kwargs,
        )
        self.query = nn.Linear(
            self.emb_dim, self.nheads * self.emb_kq_per_head, bias=use_bias
        )
        self.key = nn.Linear(
            self.emb_dim, self.kvheads * self.emb_kq_per_head, bias=use_bias
        )
        self.value = nn.Linear(
            self.emb_dim, self.kvheads * self.emb_v_per_head, bias=use_bias
        )

    def reset_parameters(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.trunc_normal_(m.weight, mean=0.0, std=0.02)
                if self.use_bias:
                    m.bias.data.zero_()

    def forward(
        self, q: torch.Tensor, k: Optional[torch.Tensor], v: Optional[torch.Tensor]
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        if k is None and v is None:
            k = q
            v = q
        elif k is None or v is None:
            raise ValueError(
                "both k and v must either be given as tensors or both None"
            )

        # b x h x qlen x ds
        queries = self.query(q)
        keys = self.key(k)
        values = self.value(v)
        return queries, keys, values


class FusedQKV(QKV):
    """
    Fused Weights implementation of QKV
    """

    def __init__(
        self,
        emb_dim: int,
        nheads: int,
        kvheads: int,
        emb_kq_per_head: int,
        emb_v_per_head: int,
        use_bias: bool,
        *args,
        **kwargs,
    ):
        super().__init__(
            emb_dim,
            nheads,
            kvheads,
            emb_kq_per_head,
            emb_v_per_head,
            use_bias,
            *args,
            **kwargs,
        )
        self.splits = [
            self.nheads * self.emb_kq_per_head,
            self.kvheads * self.emb_kq_per_head,
            self.kvheads * self.emb_v_per_head,
        ]

        self.qkv_fused = nn.Linear(
            self.emb_dim,
            sum(self.splits),
            bias=self.use_bias,
        )

    def reset_parameters(self):
        nn.init.trunc_normal_(self.qkv_fused.weight, mean=0.0, std=0.02)
        if self.use_bias:
            self.qkv_fused.bias.data.zero_()

    def forward(
        self, q: torch.Tensor, k: Optional[torch.Tensor], v: Optional[torch.Tensor]
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        if (k is None and v is None) or (k is q and v is q):
            qkv = q
        else:
            raise ValueError("q, k, and v must be the same or k and v must be None")
        return self.qkv_fused(qkv).split(self.splits, dim=-1)


class MultiHeadAttention(nn.Module):
    """
    Performs multi-headed self- or cross-attention, with optional attention masking.
    ...
    Args
    ----
    emb_dim : int
        Latent dimensionality of input and output tensors.
    emb_kq : int
        Latent dimensionality of each head in key and query projections (attention dimension).
    emb_v : int
        Latent dimensionality of each head in value projection (mixing dimension).
    nheads : int
        Number of attention heads.
    p_dropout : float|None
        Dropout probability. Must be in range [0,1]. If 0 or None, dropout will not be used.
    use_bias : bool
        Include bias terms in fully-connected sublayers?
    fused: bool
        if True, qkv weights will be fused, otherwise qkv weights will be unfused
    """

    def __init__(
        self,
        emb_dim,
        emb_kq,
        emb_v,
        nheads,
        kvheads,
        p_dropout=None,
        use_bias=False,
        position_encoder: Optional[PositionEncoder] = None,
        fused: bool = True,
    ):
        super(MultiHeadAttention, self).__init__()
        self.nheads = nheads
        self.kvheads = kvheads
        self.emb_dim = emb_dim
        self.emb_kq_per_head = emb_kq
        self.emb_v_per_head = emb_v
        self.p_dropout = p_dropout if p_dropout is not None else 0.0
        self.use_bias = use_bias
        self.fused = fused

        self.in_proj: QKV = (FusedQKV if self.fused else UnfusedQKV)(
            self.emb_dim,
            self.nheads,
            self.kvheads,
            self.emb_kq_per_head,
            self.emb_v_per_head,
            self.use_bias,
        )

        self.dense = nn.Linear(
            self.nheads * self.emb_v_per_head, self.emb_dim, bias=use_bias
        )
        if self.p_dropout:
            self.attn_dropout = nn.Dropout(self.p_dropout)
        self.position_encoder = position_encoder
        self.ln_k = LayerNormParameterized(
            emb_kq, use_high_precision_pow=True
        )
        self.ln_v = LayerNormParameterized(emb_v, use_high_precision_pow=True)

        self.inp_len = 0
        self.plan = None
        self.imap = None

        # fmap = {8 - i: 64 - (i) ** 2 for i in range(8)}
        # fmap.pop(8)
        # fmap.pop(7)
        fmap = {
            1:26,
            2:50,
            3:71,
            4:89,
            5:104,
            6:116,
            7:124
        }
        self.fmap = fmap
        self.cache_size = 128 # 64
        
        self.register_buffer("ringmap", torch.arange(self.cache_size).int())
        
        self.w = nn.Linear(self.emb_dim, self.kvheads, bias=False)

        self.step = 0

    def reset_parameters(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.trunc_normal_(m.weight, mean=0.0, std=0.02)
                if self.use_bias:
                    m.bias.data.zero_()
            elif isinstance(m, LayerNormParameterized) or isinstance(m, QKV):
                m.reset_parameters()

    def scan(self, x, plan, inds, i, w):
        """
        Takes input x of shape [b n ...] and scan plan, computes recursive sums, and
        extracts cache values into the dimension specified by i > 1.
        Final output shape is therefore [b n ... c ...] with c the cache size.
        Applies specified LN to cache, so LN size should match x.size(-1).
        """
        assert i > 1, "Output must maintain batch and seqlen in first two dimensions"
        s = x.size()
        ws = w.size()
        # Plan and inds are formed, construct cache via recursive sums
        cache = [None for _ in plan]
        cache[1] = nn.functional.pad(x.view(s[0], s[1], -1), (0, 0, 1, 0)).view(
            s[0], s[1] + 1, *s[2:]
        )  # b n ...
        weights = [None for _ in plan]
        weights[1] = nn.functional.pad(w.view(s[0], s[1], -1), (0,0,1,0), value=-1000).view(
            s[0], s[1] + 1, *ws[2:]
        )
        for j in range(2, len(cache)):
            weights[j] = weights[j-1].index_select(1, plan[j].view(-1)).view(s[0], -1, 2, *ws[2:])
            weights_ = weights[j].softmax(dim=2).unsqueeze(-1)
            weights[j] = weights[j].logsumexp(2)
            cache[j] = (
                cache[j - 1]
                .index_select(1, plan[j].view(-1))
                .view(s[0], -1, 2, *s[2:])
            )
            cache[j] = cache[j].mul(weights_).sum(2)

        # Gather cache    
        cache = torch.cat(cache[1:], dim=1)  # b n' ...
        cache = cache.unsqueeze(i).expand(
            *[-1] * i, inds.size(-1), *[-1] * (len(s) - i)
        )  # b n' ... h ...
        inds_ = inds.view(
            1, inds.size(0), *[1] * (i - 2), inds.size(1), *[1] * (len(s) - i)
        )  # 1 n 111 h 111
        inds_ = inds_.expand(s[0], -1, *s[2:i], -1, *s[i:])  # b n ... h ...
        cache = cache.gather(1, inds_)  # b n ... h ...
        
        # Gather final weights
        weights = torch.cat(weights[1:], dim=1)  # b n' ...
        inds_ = inds[-1]  # h
        weights = weights.index_select(1, inds_)  # b h ...
        weights = weights.view(ws[0],weights.size(1),-1).transpose(1,2)  # b -1 h
        weights = weights.reshape(ws[0], 1, *ws[2:], -1)  # b 1 ... h

        return cache, weights
    
    def advance(self, cache, weights, x, w, update_ringmap=True):
        # cache: b h c d
        # weights: b h c
        # x: b h d
        # w: b h
        c = self.cache_size
        powers = (2**torch.arange(10, device=cache.device))
        ilevel = (self.step%powers).sub(powers-1).sign().add(1).sum().item()
        key = self.fmap.get(ilevel, c)
        
        if key == c:
            cache[:,:,self.ringmap[-1]] = x
            weights[:,:,self.ringmap[-1]] = w
            if update_ringmap:
                self.ringmap = self.ringmap.roll(1)
        else:
            w_ = weights[:,:,self.ringmap[key-1:key+1]]  # b h 2
            c_ = cache[:,:,self.ringmap[key-1:key+1]]  # b h 2 d
            mix = w_.sub(w_.logsumexp(2, True)).exp()
            cache[:,:,self.ringmap[key]] = c_.mul(mix.unsqueeze(3)).sum(2)
            weights[:,:,self.ringmap[key]] = w_.mul(mix).sum(2)
            cache[:,:,self.ringmap[key-1]] = x
            weights[:,:,self.ringmap[key-1]] = w
            if update_ringmap:
                self.ringmap[:key] = self.ringmap[:key].roll(1)
        return cache, weights

    def forward(
        self,
        q: torch.Tensor,
        k: Optional[torch.Tensor] = None,
        v: Optional[torch.Tensor] = None,
        mask: Optional[Tensor] = None,
        position_ids=None,
        attn_algorithm=None,
        past_key_value_state: Optional[Tuple[Tensor, Tensor]] = None,
        use_cache=False,
        is_self=True,
        is_causal_mask=False,
    ):
        """
        past_key_value_state: tuple
            the cache to be used in attention of the form (<self/cross>_key, <self/cross>_value)
        position_ids: Optional[torch.LongTensor]
            The position of each of the tokens encoded in q and k. Used for RoPE embeddings
        use_cache: bool
            if True, the kv states for self/cross attention will be saved, otherwise they will not be saved
        is_self: bool
            if True, this will perform self attention, otherwise this will perform cross attention. Note: This will
            only be used in the case that use_cache=True. This may be removed in future

        Returns
        -------
        tensor or tuple
            If use_cache=False, only the hidden state will be returned as a tensor. If use_cache=True, a tuple will be
            returned in the form (hidden_state, cache) where hidden_state is a tensor and cache is of the form specified
            in past_key_value_state
        """

        batch_size, q_len, _ = q.size()
        q_out, k_out, v_out = self.in_proj(q, k, v)
        
        # note: transposes will be moved in a later PR to fix dis-contiguous tensor issues
        queries = q_out.view(batch_size, q_len, self.nheads, self.emb_kq_per_head)
        keys = k_out.view(batch_size, q_len, self.kvheads, self.emb_kq_per_head)
        values = v_out.view(batch_size, q_len, self.kvheads, self.emb_v_per_head)

        # You want to apply rotary embeddings pre-cache
        if self.position_encoder is not None:
            if q_len == 1 and position_ids is None:
                position_ids = torch.ones(batch_size, q_len, device=q.device).mul(self.step).int()
            queries, keys = self.position_encoder.adjusted_qk(
                queries, keys, position_ids, past_key_value_state, use_cache
            )
        
        queries = queries / (self.emb_kq_per_head**0.5)  # b l h d
        w = self.w(q)  # b l h

        # Advance caches
        if q_len == 1:
            past_key_value_state[0], past_key_value_state[2] = self.advance(
                past_key_value_state[0][:,0], 
                past_key_value_state[2][:,0], 
                keys.squeeze(1),  # b h d
                w.squeeze(1),  # b h 
                False
            )
            past_key_value_state[1], past_key_value_state[3] = self.advance(
                past_key_value_state[1][:,0],
                past_key_value_state[3][:,0],
                values.squeeze(1),  # b h d
                w.squeeze(1),  # b h
                True
            )
            past_key_value_state = [x.unsqueeze(1) for x in past_key_value_state]
        else:
            # Reset caches
            past_key_value_state = [None,] * 4
            self.step = 0
            self.ringmap = torch.ones_like(self.ringmap).cumsum(0).sub(1)
            # Generate plan by truncating master plan - generate new master if needed
            if q_len > self.inp_len:
                self.inp_len = 2**(q_len-1).bit_length()
                self.plan, self.imap = get_scan_plan(q.device, self.inp_len, self.fmap, self.cache_size)
            plan, imap = shrink_plan(self.plan, self.imap, q_len)
            # Scan
            past_key_value_state[0], past_key_value_state[2] = self.scan(keys, plan, imap, 3, w)
            past_key_value_state[1], past_key_value_state[3] = self.scan(values, plan, imap, 3, w)

        # Advance step counter
        self.step += q_len

        # Do attention against caches
        # q: b l h e d
        # k: b l h c d
        # v: b l h c d
        keys = self.ln_k(past_key_value_state[0])
        values = self.ln_v(past_key_value_state[1])
        queries = queries.view(batch_size, q_len, self.kvheads, -1, self.emb_kq_per_head)
        attn = queries.matmul(keys.transpose(3,4))  # b l h e c
        attn = attn.softmax(4)
        attn = attn.matmul(values)  # b l h e d

        attn = attn.reshape(batch_size, q_len, self.nheads * self.emb_v_per_head)
        out = self.dense(attn)

        if use_cache:
            return out, [x[:,-1:] for x in past_key_value_state]
        else:
            return out


class TPMultiHeadAttention(MultiHeadAttention, TPModule):
    """
    Performs multi-headed self- or cross-attention, with optional attention masking.
    This subclass adds support for Tensor Parallel
    ...
    Args
    ----
    Check MultiHeadAttention for up-to-date docs

    world_size: int
        the number of processes running this model in TP
    rank: int
        the index of this process wrt to the rest running the model in TP
    """

    def __init__(
        self,
        emb_dim,
        emb_kq,
        emb_v,
        nheads,
        kvheads,
        p_dropout=None,
        use_bias=False,
        position_encoder: Optional[PositionEncoder] = None,
        fused: bool = True,
        group: Optional[ProcessGroup] = None,
    ):
        assert torch.distributed.is_initialized()

        rank, world_size = distributed.rank_and_world(group)
        assert (
            nheads % world_size == 0
        ), "The number of heads must be divisible by world size"
        assert (kvheads >= world_size and kvheads % world_size == 0) or (
            kvheads < world_size and world_size % kvheads == 0
        ), "the kv heads must be divisible by the world size or the world size must be divisible by kv heads"
        MultiHeadAttention.__init__(
            self,
            emb_dim,
            emb_kq,
            emb_v,
            nheads // world_size,
            (kvheads // world_size) if kvheads >= world_size else 1,
            p_dropout,
            use_bias,
            position_encoder,
            fused,
        )
        self.pre_tp_nheads = nheads
        self.pre_tp_kvheads = kvheads
        self.setup_tp(rank, world_size)

    def load_weights(
        self,
        tensor_values: Dict[str, torch.Tensor],
    ):
        used_keys: Set[str] = set()
        dense_weight = self._get_sd_weight(
            tensor_values, used_keys, ["dense", "weight"]
        )
        if self.use_bias:
            dense_bias = self._get_sd_weight(
                tensor_values, used_keys, ["dense", "bias"]
            )

        # 1. Grab the weights from tensor_values
        if self.fused:
            qkv_weight = self._get_sd_weight(
                tensor_values, used_keys, ["qkv_fused", "weight"]
            )
            if self.use_bias:
                qkv_bias = self._get_sd_weight(
                    tensor_values, used_keys, ["qkv_fused", "bias"]
                )

            # 2. Raise exceptions
            if len(tensor_values) > (4 if self.use_bias else 2):
                unused_keys = set(tensor_values.keys()).difference(used_keys)
                raise AttributeError(f"Unused weight(s): {', '.join(unused_keys)}")

            # 3. Load and shard the weights
            # The number in max_partition_sizes will signify the largest world size
            # til we need to duplicate.  For instance if we have nheads=16 and
            # world_size=32, then first 2 ranks will get first 1/16th of query
            self.sharded_copy(
                self.in_proj.qkv_fused.weight,
                qkv_weight,
                0,
                [self.pre_tp_nheads, self.pre_tp_kvheads, self.pre_tp_kvheads],
            )
            self.sharded_copy(self.dense.weight, dense_weight, 1, [self.world_size])
            if self.use_bias:
                self.sharded_copy(
                    self.in_proj.qkv_fused.bias,
                    qkv_bias,
                    0,
                    [self.pre_tp_nheads, self.pre_tp_kvheads, self.pre_tp_kvheads],
                )
                self.sharded_copy(
                    self.dense.bias, dense_bias, 1, [self.world_size], False
                )

        else:
            query_weight = self._get_sd_weight(
                tensor_values, used_keys, ["query", "weight"]
            )
            key_weight = self._get_sd_weight(
                tensor_values, used_keys, ["key", "weight"]
            )
            value_weight = self._get_sd_weight(
                tensor_values, used_keys, ["value", "weight"]
            )

            if self.use_bias:
                query_bias = self._get_sd_weight(
                    tensor_values, used_keys, ["query", "bias"]
                )
                key_bias = self._get_sd_weight(
                    tensor_values, used_keys, ["key", "bias"]
                )
                value_bias = self._get_sd_weight(
                    tensor_values, used_keys, ["value", "bias"]
                )

            # 2. Raise exceptions
            if len(tensor_values) > (8 if self.use_bias else 4):
                unused_keys = set(tensor_values.keys()).difference(used_keys)
                raise AttributeError(f"Unused weight(s): {', '.join(unused_keys)}")

            # 3. Load and shard the weights
            # The number in max_partition_sizes will signify the largest world size
            # til we need to duplicate.  For instance if we have nheads=16 and
            # world_size=32, then first 2 ranks will get first 1/16th of query
            self.sharded_copy(
                self.in_proj.query.weight, query_weight, 0, [self.pre_tp_nheads]
            )
            self.sharded_copy(
                self.in_proj.key.weight, key_weight, 0, [self.pre_tp_kvheads]
            )
            self.sharded_copy(
                self.in_proj.value.weight, value_weight, 0, [self.pre_tp_kvheads]
            )
            self.sharded_copy(self.dense.weight, dense_weight, 1, [self.world_size])
            if self.use_bias:
                self.sharded_copy(
                    self.in_proj.query.bias, query_bias, 0, [self.pre_tp_nheads]
                )
                self.sharded_copy(
                    self.in_proj.key.bias, key_bias, 0, [self.pre_tp_kvheads]
                )
                self.sharded_copy(
                    self.in_proj.value.bias, value_bias, 0, [self.pre_tp_kvheads]
                )
                self.sharded_copy(
                    self.dense.bias, dense_bias, 1, [self.world_size], False
                )

    @staticmethod
    def import_module(
        mha: MultiHeadAttention, group: ProcessGroup
    ) -> "TPMultiHeadAttention":
        tp_mha = TPMultiHeadAttention(
            emb_dim=mha.emb_dim,
            emb_kq=mha.emb_kq_per_head,
            emb_v=mha.emb_v_per_head,
            nheads=mha.nheads,
            kvheads=mha.kvheads,
            p_dropout=mha.p_dropout,
            use_bias=mha.use_bias,
            position_encoder=mha.position_encoder,
            group=group,
            fused=mha.fused,
        )
        return tp_mha

    def _copy_to_tp_region(
        self,
        q: torch.Tensor,
        k: Optional[torch.Tensor] = None,
        v: Optional[torch.Tensor] = None,
    ):
        if (k is None and v is None) or (k is q and v is q):
            q_par = copy_to_tensor_model_parallel_region(q)
            if self.fused:
                k_par = None
                v_par = None
            else:
                k_par = copy_to_tensor_model_parallel_region(k)
                v_par = copy_to_tensor_model_parallel_region(v)
        else:
            raise ValueError(
                "both k and v must either be given as tensors or both None"
            )

        return q_par, k_par, v_par

    def forward(
        self,
        q,
        k=None,
        v=None,
        mask=None,
        position_ids=None,
        attn_algorithm=None,
        past_key_value_state=None,
        use_cache=False,
        is_self=True,
        is_causal_mask=False,
    ):
        """
        Check MultiHeadAttention for up-to-date arguments and docs
        """

        q_par, k_par, v_par = self._copy_to_tp_region(q, k, v)

        out_par = MultiHeadAttention.forward(
            self,
            q_par,
            k_par,
            v_par,
            mask,
            position_ids,
            attn_algorithm,
            past_key_value_state,
            use_cache,
            is_self,
            is_causal_mask,
        )

        # if use_cache=True, we return the hidden_state as well as the kv cache.
        # We only reduce the output, and keep the cache thread-local
        if use_cache:
            out = reduce_from_tensor_model_parallel_region(out_par[0])
            return out, out_par[1]
        else:
            out = reduce_from_tensor_model_parallel_region(out_par)
            return out
